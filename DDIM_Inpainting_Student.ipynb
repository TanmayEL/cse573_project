{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DDIM Inpainting on MNIST\n",
        "**Learning:** Build a DDIM pipeline for center-hole inpainting: timestep-conditioned U-Net, DDIM sampling, hole-only PSNR/L1.  \n",
        "**Goal:** Train on masked MNIST, then sample reconstructions and evaluate only inside the mask.  \n",
        "> **Tip:** Use **Google Colab (GPU)** and keep batch size modest to avoid OOM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Prerequisites (Colab GPU)\n",
        "- In Colab: **Runtime → Change runtime type → Hardware accelerator → GPU → Save**.\n",
        "- CPU works but training will be **much slower**.\n",
        "- This notebook installs only minimal packages.\n",
        "\n",
        "> If you prefer Kaggle: enable GPU in **Settings → Accelerator → GPU**.\n",
        "\n",
        "**Quick GPU check (run once):**\n",
        "```python\n",
        "import torch\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available(): print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f6d5a82",
      "metadata": {},
      "source": [
        "## 1. Install minimal dependencies\n",
        "\n",
        "- **Colab users:** PyTorch is already installed—**don’t reinstall `torch`/`torchvision`**. Install only extras below.  \n",
        "- **Non-Colab users:** Use the pinned install shown after the Colab snippet.\n",
        "\n",
        "**Colab (recommended):**\n",
        "```bash\n",
        "!pip -q install -U tqdm matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88f6f65b",
      "metadata": {},
      "source": [
        "## 2. Check GPU\n",
        "\n",
        "Run this cell to verify that CUDA is available (Colab GPU recommended):\n",
        "\n",
        "```python\n",
        "import torch, platform\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA available?\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"Running on CPU — training will be much slower.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30a63f8f",
      "metadata": {},
      "source": [
        "## 3. Quick Equations (reference)\n",
        "\n",
        "**Forward noising**\n",
        "$$\n",
        "x_t=\\sqrt{\\bar{\\alpha}_t}\\,x_0+\\sqrt{1-\\bar{\\alpha}_t}\\,\\varepsilon,\\qquad \\varepsilon\\sim\\mathcal N(0,I)\n",
        "$$\n",
        "\n",
        "**Cosine schedule — beta\\_t from abar(t)**\n",
        "$$\n",
        "\\bar{\\alpha}(t)=\\cos^2\\!\\left(\\frac{t/T+s}{1+s}\\cdot\\frac{\\pi}{2}\\right)\n",
        "$$\n",
        "$$\n",
        "\\beta_t=\\min\\!\\left(0.999,\\;1-\\frac{\\bar{\\alpha}(t+1)}{\\bar{\\alpha}(t)}\\right)\n",
        "$$\n",
        "\n",
        "**Prediction-space mapping**\n",
        "- **pred = `eps`**\n",
        "$$\n",
        "\\hat\\varepsilon=\\text{out},\\qquad\n",
        "\\hat x_0=\\frac{x_t-\\sqrt{1-\\bar{\\alpha}_t}\\,\\hat\\varepsilon}{\\sqrt{\\bar{\\alpha}_t}}\n",
        "$$\n",
        "\n",
        "- **pred = `x0`**\n",
        "$$\n",
        "\\hat x_0=\\text{out},\\qquad\n",
        "\\hat\\varepsilon=\\frac{x_t-\\sqrt{\\bar{\\alpha}_t}\\,\\hat x_0}{\\sqrt{1-\\bar{\\alpha}_t}}\n",
        "$$\n",
        "\n",
        "- **pred = `v`**\n",
        "$$\n",
        "\\hat x_0=\\sqrt{\\bar{\\alpha}_t}\\,x_t-\\sqrt{1-\\bar{\\alpha}_t}\\,\\text{out},\\qquad\n",
        "\\hat\\varepsilon=\\sqrt{1-\\bar{\\alpha}_t}\\,x_t+\\sqrt{\\bar{\\alpha}_t}\\,\\text{out}\n",
        "$$\n",
        "\n",
        "**DDIM step** (optional noise eta)\n",
        "$$\n",
        "\\sigma_t=\\eta\\cdot\\sqrt{\\frac{1-\\bar{\\alpha}_{t'}}{1-\\bar{\\alpha}_t}\\left(1-\\frac{\\bar{\\alpha}_t}{\\bar{\\alpha}_{t'}}\\right)}\n",
        "$$\n",
        "$$\n",
        "x_{t'}=\\sqrt{\\bar{\\alpha}_{t'}}\\,\\hat x_0+\\sqrt{1-\\bar{\\alpha}_{t'}-\\sigma_t^{2}}\\,\\hat\\varepsilon+\\mathbf{1}_{\\eta>0}\\,\\sigma_t z\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d11714c0",
      "metadata": {},
      "source": [
        "## 4. Utilities & Data _(GIVEN — do not modify)_\n",
        "\n",
        "Helper functions used throughout the notebook:\n",
        "- **Repro & device:** `seed_all`, `dev`\n",
        "- **I/O & viz:** `ensure_dir`, `save_grid`, `safe_torch_load`\n",
        "- **Data:** `get_mnist_loader` (MNIST → 32×32, normalized to \\([-1,1]\\))\n",
        "- **Masking:** `make_center_box_mask` (zeros at center box = hole)\n",
        "- **Metrics (hole-only):** `psnr_on_mask`, `l1_on_mask`\n",
        "\n",
        "> Note: `seed_all` sets `cudnn.benchmark=True` for speed (slightly non-deterministic). If you need **strict reproducibility**, temporarily set it to `False`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, math, json, random\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils as vutils\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# Utils\n",
        "# ----------------------------\n",
        "def seed_all(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def dev():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def ensure_dir(p):\n",
        "    os.makedirs(p, exist_ok=True); return p\n",
        "\n",
        "def save_grid(x, path, nrow=8, rng=(-1,1), dpi=220):\n",
        "    ensure_dir(os.path.dirname(path) or \".\")\n",
        "    grid = vutils.make_grid(x, nrow=nrow, normalize=True, value_range=rng)\n",
        "    plt.figure(figsize=(5,5)); plt.axis(\"off\")\n",
        "    arr = grid.detach().cpu().numpy().transpose(1,2,0)\n",
        "    if arr.shape[2] == 1:\n",
        "        arr = np.repeat(arr, 3, axis=2)\n",
        "    plt.imshow(arr); plt.tight_layout(); plt.savefig(path, dpi=dpi); plt.close()\n",
        "\n",
        "def safe_torch_load(path, map_location=\"cpu\"):\n",
        "    try:\n",
        "        return torch.load(path, map_location=map_location, weights_only=True)  # PyTorch ≥2.4\n",
        "    except TypeError:\n",
        "        return torch.load(path, map_location=map_location)\n",
        "\n",
        "def get_mnist_loader(batch_size=128, num_workers=2):\n",
        "    tfm = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((32,32), antialias=True),\n",
        "        transforms.Normalize((0.5,), (0.5,)),  # [-1,1]\n",
        "    ])\n",
        "    tr = datasets.MNIST('./data', train=True,  transform=tfm, download=True)\n",
        "    te = datasets.MNIST('./data', train=False, transform=tfm, download=True)\n",
        "    train = DataLoader(tr, batch_size, shuffle=True,  num_workers=num_workers, drop_last=True)\n",
        "    test  = DataLoader(te, batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
        "    return train, test, (1,32,32)\n",
        "\n",
        "# ----------------------------\n",
        "# Masks (centered, fixed size)\n",
        "# ----------------------------\n",
        "def make_center_box_mask(B, H, W, box=12):\n",
        "    m = torch.ones(B,1,H,W)\n",
        "    hs, ws = H//2, W//2\n",
        "    h0, h1 = hs - box//2, hs + box//2\n",
        "    w0, w1 = ws - box//2, ws + box//2\n",
        "    m[:,:,h0:h1,w0:w1] = 0.0\n",
        "    return m\n",
        "\n",
        "# ----------------------------\n",
        "# Metrics (hole region only)\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def psnr_on_mask(pred, target, known_mask, eps=1e-8):\n",
        "    hole = (1.0 - known_mask)\n",
        "    N = hole.sum().clamp_min(1.0)\n",
        "    p = (pred+1)/2; t = (target+1)/2\n",
        "    mse = ((p - t)**2 * hole).sum() / N\n",
        "    return float(10.0 * torch.log10(1.0 / (mse + eps)))\n",
        "\n",
        "@torch.no_grad()\n",
        "def l1_on_mask(pred, target, known_mask):\n",
        "    hole = (1.0 - known_mask)\n",
        "    N = hole.sum().clamp_min(1.0)\n",
        "    return float(((pred - target).abs() * hole).sum() / N)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8bdcf64",
      "metadata": {},
      "source": [
        "## 5. Model & Diffusion — YOUR IMPLEMENTATIONS (TODO)\n",
        "\n",
        "You’ll implement the core pieces of the model and sampler. Keep interfaces unchanged; only fill the **TODO** parts. Use the **Quick Equations** section above as reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sinusoidal_embed(t: torch.Tensor, dim: int) -> torch.Tensor:\n",
        "    r\"\"\"\n",
        "    ### TODO: implement sinusoidal timestep embedding ϕ(t) ∈ ℝ^{B×dim}.\n",
        "\n",
        "    Let H = ⌊dim/2⌋, and frequencies\n",
        "      f_k = exp( - (log 10000) * k / max(1, H-1) ),  k = 0..H-1.\n",
        "\n",
        "    Define\n",
        "      ϕ(t) = [ sin(t f_0), ..., sin(t f_{H-1}),  cos(t f_0), ..., cos(t f_{H-1}) ].\n",
        "    If dim is odd, append one zero column.\n",
        "\n",
        "    Inputs:\n",
        "      t: (B,) Long/float tensor (will cast to float internally)\n",
        "      dim: embedding size\n",
        "    Output:\n",
        "      (B, dim) on same device as t.\n",
        "    \"\"\"\n",
        "    # ====== Implement here ======\n",
        "    t = t.float()\n",
        "    H = dim // 2\n",
        "    # Compute frequencies\n",
        "    k = torch.arange(H, dtype=torch.float32, device=t.device)\n",
        "    log_10000 = math.log(10000.0)\n",
        "    freqs = torch.exp(-log_10000 * k / max(1, H - 1))\n",
        "    # Compute embeddings\n",
        "    emb = t[:, None] * freqs[None, :]  # (B, H)\n",
        "    sin_emb = torch.sin(emb)\n",
        "    cos_emb = torch.cos(emb)\n",
        "    # Concatenate sin and cos\n",
        "    result = torch.cat([sin_emb, cos_emb], dim=-1)  # (B, 2*H)\n",
        "    # If dim is odd, append zero column\n",
        "    if dim % 2 == 1:\n",
        "        result = torch.cat([result, torch.zeros_like(result[:, :1])], dim=-1)\n",
        "    return result\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    r\"\"\"\n",
        "    ### TODO: Residual block with time embedding injection\n",
        "\n",
        "    Given input x ∈ ℝ^{B×C×H×W} and time embedding e ∈ ℝ^{B×E},\n",
        "    compute:\n",
        "      h = x + MLP(e)[:, :, None, None]\n",
        "      h = GN(C) → SiLU → Conv2d(C→C, 3×3, s=1, p=1)\n",
        "      h = Dropout2d(p=dropout)\n",
        "      h = GN(C) → SiLU → Conv2d(C→C, 3×3, s=1, p=1)\n",
        "      out = x + h\n",
        "\n",
        "    Use GroupNorm with 'groups' (cap to divisors of C; at least 1).\n",
        "    \"\"\"\n",
        "    def __init__(self, ch: int, emb: int, dropout: float=0.1, groups: int=8):\n",
        "        super().__init__()\n",
        "        # ====== Implement here ======\n",
        "        # Cap groups to divisors of ch, at least 1\n",
        "        actual_groups = min(groups, ch)\n",
        "        for g in range(actual_groups, 0, -1):\n",
        "            if ch % g == 0:\n",
        "                actual_groups = g\n",
        "                break\n",
        "        \n",
        "        self.mlp = nn.Linear(emb, ch)\n",
        "        self.norm1 = nn.GroupNorm(actual_groups, ch)\n",
        "        self.conv1 = nn.Conv2d(ch, ch, 3, padding=1)\n",
        "        self.dropout = nn.Dropout2d(dropout)\n",
        "        self.norm2 = nn.GroupNorm(actual_groups, ch)\n",
        "        self.conv2 = nn.Conv2d(ch, ch, 3, padding=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, e: torch.Tensor) -> torch.Tensor:\n",
        "        # ====== Implement here ======\n",
        "        h = x + self.mlp(e)[:, :, None, None]\n",
        "        h = self.norm1(h)\n",
        "        h = F.silu(h)\n",
        "        h = self.conv1(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.norm2(h)\n",
        "        h = F.silu(h)\n",
        "        h = self.conv2(h)\n",
        "        return x + h\n",
        "\n",
        "\n",
        "class SelfAttention2d(nn.Module):\n",
        "    r\"\"\"\n",
        "    ### TODO: Single-head self-attention at spatial resolution (e.g., 8×8).\n",
        "\n",
        "    Normalize with GroupNorm(8, C).\n",
        "    Compute q,k,v via 1×1 convs (C→C).\n",
        "    Shapes:\n",
        "      x: (B,C,H,W)\n",
        "      q: (B,HW,C)   from (B,C,H,W) → (B,C,HW) → (B,HW,C)\n",
        "      k: (B,C,HW)\n",
        "      v: (B,HW,C)\n",
        "      attn = softmax( (q @ k)/√C , dim=-1 )   → (B,HW,HW)\n",
        "      out  = (attn @ v) → (B,HW,C) → (B,C,H,W) → proj 1×1 → residual add.\n",
        "    \"\"\"\n",
        "    def __init__(self, ch: int):\n",
        "        super().__init__()\n",
        "        # ====== Implement here ======\n",
        "        self.norm = nn.GroupNorm(8, ch)\n",
        "        self.q_conv = nn.Conv2d(ch, ch, 1)\n",
        "        self.k_conv = nn.Conv2d(ch, ch, 1)\n",
        "        self.v_conv = nn.Conv2d(ch, ch, 1)\n",
        "        self.proj = nn.Conv2d(ch, ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ====== Implement here ======\n",
        "        B, C, H, W = x.shape\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "        \n",
        "        # Compute q, k, v\n",
        "        q = self.q_conv(x)  # (B, C, H, W)\n",
        "        k = self.k_conv(x)  # (B, C, H, W)\n",
        "        v = self.v_conv(x)  # (B, C, H, W)\n",
        "        \n",
        "        # Reshape for attention\n",
        "        q = q.view(B, C, H * W).transpose(1, 2)  # (B, HW, C)\n",
        "        k = k.view(B, C, H * W)  # (B, C, HW)\n",
        "        v = v.view(B, C, H * W).transpose(1, 2)  # (B, HW, C)\n",
        "        \n",
        "        # Attention\n",
        "        attn = torch.softmax((q @ k) / math.sqrt(C), dim=-1)  # (B, HW, HW)\n",
        "        out = attn @ v  # (B, HW, C)\n",
        "        \n",
        "        # Reshape back\n",
        "        out = out.transpose(1, 2).view(B, C, H, W)  # (B, C, H, W)\n",
        "        out = self.proj(out)\n",
        "        \n",
        "        return residual + out\n",
        "\n",
        "\n",
        "class UNetDeep(nn.Module):\n",
        "    r\"\"\"\n",
        "    ### TODO: Implement the UNet backbone with time-conditioning and mid attention.\n",
        "\n",
        "    Input channels depend on flags:\n",
        "      - Base: [x_t, m, y] → 3 channels\n",
        "      - + self-conditioning: add x0_sc → +1 channel (total 4)\n",
        "      - + coord_conv: add (coord_x, coord_y) → +2 channels (total 5 or 6)\n",
        "\n",
        "    Spec (assume base=B):\n",
        "      t-MLP: Linear(E→2E) → SiLU → Linear(2E→E)\n",
        "      Down:\n",
        "        inp:  Conv2d(in_ch→B, 3×3,1,1)\n",
        "        rb0:  ResBlock(B,E)\n",
        "        d1 :  Conv2d(B→2B, 4×4,2,1)\n",
        "        rb1:  ResBlock(2B,E)\n",
        "        d2 :  Conv2d(2B→4B, 4×4,2,1)\n",
        "        rb2:  ResBlock(4B,E)\n",
        "      Mid (8×8):\n",
        "        mid1: ResBlock(4B,E)\n",
        "        attn: SelfAttention2d(4B)\n",
        "        mid2: ResBlock(4B,E)\n",
        "      Up:\n",
        "        u1 :  ConvTranspose2d(4B→2B, 4×4,2,1)\n",
        "        red1: Conv2d(2B+2B→2B, 1×1)\n",
        "        rb3:  ResBlock(2B,E)\n",
        "        u2 :  ConvTranspose2d(2B→B, 4×4,2,1)\n",
        "        red2: Conv2d(B+B→B, 1×1)\n",
        "        rb4:  ResBlock(B,E)\n",
        "      Out:\n",
        "        Conv2d(B→1, 3×3,1,1)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=3, base=96, emb=384, out_ch=1, dropout=0.1, use_attn=True):\n",
        "        super().__init__()\n",
        "        # ====== Implement here ======\n",
        "        self.base = base\n",
        "        self.emb = emb\n",
        "        \n",
        "        # Time embedding MLP\n",
        "        self.t_mlp = nn.Sequential(\n",
        "            nn.Linear(emb, 2 * emb),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(2 * emb, emb)\n",
        "        )\n",
        "        \n",
        "        # Input projection\n",
        "        self.inp = nn.Conv2d(in_ch, base, 3, padding=1)\n",
        "        \n",
        "        # Down path\n",
        "        self.rb0 = ResBlock(base, emb, dropout)\n",
        "        self.d1 = nn.Conv2d(base, 2 * base, 4, stride=2, padding=1)\n",
        "        self.rb1 = ResBlock(2 * base, emb, dropout)\n",
        "        self.d2 = nn.Conv2d(2 * base, 4 * base, 4, stride=2, padding=1)\n",
        "        self.rb2 = ResBlock(4 * base, emb, dropout)\n",
        "        \n",
        "        # Mid path\n",
        "        self.mid1 = ResBlock(4 * base, emb, dropout)\n",
        "        self.attn = SelfAttention2d(4 * base) if use_attn else nn.Identity()\n",
        "        self.mid2 = ResBlock(4 * base, emb, dropout)\n",
        "        \n",
        "        # Up path\n",
        "        self.u1 = nn.ConvTranspose2d(4 * base, 2 * base, 4, stride=2, padding=1)\n",
        "        self.red1 = nn.Conv2d(2 * base + 2 * base, 2 * base, 1)\n",
        "        self.rb3 = ResBlock(2 * base, emb, dropout)\n",
        "        self.u2 = nn.ConvTranspose2d(2 * base, base, 4, stride=2, padding=1)\n",
        "        self.red2 = nn.Conv2d(base + base, base, 1)\n",
        "        self.rb4 = ResBlock(base, emb, dropout)\n",
        "        \n",
        "        # Output\n",
        "        self.out = nn.Conv2d(base, out_ch, 3, padding=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "        # ====== Implement here ======\n",
        "        # Time embedding\n",
        "        t_emb = sinusoidal_embed(t, self.emb)\n",
        "        t_emb = self.t_mlp(t_emb)\n",
        "        \n",
        "        # Down path\n",
        "        x0 = self.inp(x)\n",
        "        x0 = self.rb0(x0, t_emb)\n",
        "        \n",
        "        x1 = self.d1(x0)\n",
        "        x1 = self.rb1(x1, t_emb)\n",
        "        \n",
        "        x2 = self.d2(x1)\n",
        "        x2 = self.rb2(x2, t_emb)\n",
        "        \n",
        "        # Mid path\n",
        "        x_mid = self.mid1(x2, t_emb)\n",
        "        x_mid = self.attn(x_mid)\n",
        "        x_mid = self.mid2(x_mid, t_emb)\n",
        "        \n",
        "        # Up path\n",
        "        x_up = self.u1(x_mid)\n",
        "        x_up = torch.cat([x_up, x1], dim=1)\n",
        "        x_up = self.red1(x_up)\n",
        "        x_up = self.rb3(x_up, t_emb)\n",
        "        \n",
        "        x_up = self.u2(x_up)\n",
        "        x_up = torch.cat([x_up, x0], dim=1)\n",
        "        x_up = self.red2(x_up)\n",
        "        x_up = self.rb4(x_up, t_emb)\n",
        "        \n",
        "        # Output\n",
        "        return self.out(x_up)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Diffusion core (buffers + forward noising)\n",
        "# ----------------------------\n",
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class DCfg:\n",
        "    steps:int=400\n",
        "    beta_start:float=1e-4\n",
        "    beta_end:float=2e-2\n",
        "    beta_schedule:str=\"cosine\"  # {'linear','cosine'}\n",
        "\n",
        "def cosine_betas(T: int, s: float = 0.008):\n",
        "    r\"\"\"\n",
        "    ### TODO: implement cosine β_t from ᾱ(t)\n",
        "\n",
        "    ᾱ(t) = cos^2( ((t/T + s)/(1+s)) * π/2 ), for t in {0,...,T}\n",
        "    Then:\n",
        "      β_t = min(0.999, 1 - ᾱ(t+1)/ᾱ(t))   for t = 0..T-1\n",
        "    Return float32 tensor shape (T,)\n",
        "    \"\"\"\n",
        "    # ====== Implement here ======\n",
        "    t_vals = torch.arange(T + 1, dtype=torch.float32)\n",
        "    # Compute ᾱ(t) for t in {0,...,T}\n",
        "    a_bar_t = torch.cos((t_vals / T + s) / (1 + s) * math.pi / 2) ** 2\n",
        "    # Compute β_t for t = 0..T-1\n",
        "    beta_t = torch.minimum(\n",
        "        torch.tensor(0.999, dtype=torch.float32),\n",
        "        1.0 - a_bar_t[1:] / (a_bar_t[:-1] + 1e-8)\n",
        "    )\n",
        "    return beta_t\n",
        "\n",
        "\n",
        "class Diffusion:\n",
        "    def __init__(self, cfg: DCfg):\n",
        "        r\"\"\"\n",
        "        ### TODO: build buffers from β_t\n",
        "          α_t       = 1 - β_t\n",
        "          ᾱ_t      = ∏_{s=1}^t α_s\n",
        "          sqrt_ab   = √(ᾱ_t)\n",
        "          sqrt_1mab = √(1 - ᾱ_t)\n",
        "          sqrt_ra   = √(1/α_t)\n",
        "          post_var  = β_t * (1 - ᾱ_{t-1}) / (1 - ᾱ_t)   with ᾱ_0 = 1\n",
        "        \"\"\"\n",
        "        self.cfg = cfg\n",
        "        T = cfg.steps\n",
        "        self.T = T\n",
        "        # ====== Implement here ======\n",
        "        # Compute beta schedule\n",
        "        if cfg.beta_schedule == \"cosine\":\n",
        "            beta_t = cosine_betas(T)\n",
        "        else:  # linear\n",
        "            beta_t = torch.linspace(cfg.beta_start, cfg.beta_end, T, dtype=torch.float32)\n",
        "        \n",
        "        # α_t = 1 - β_t\n",
        "        alpha_t = 1.0 - beta_t\n",
        "        \n",
        "        # ᾱ_t = ∏_{s=1}^t α_s\n",
        "        a_bar_t = torch.cumprod(alpha_t, dim=0)\n",
        "        # Prepend 1.0 for ᾱ_0 = 1\n",
        "        a_bar_t = torch.cat([torch.tensor([1.0], dtype=torch.float32), a_bar_t])\n",
        "        \n",
        "        # sqrt_ab = √(ᾱ_t)\n",
        "        sqrt_ab = torch.sqrt(a_bar_t)\n",
        "        \n",
        "        # sqrt_1mab = √(1 - ᾱ_t)\n",
        "        sqrt_1mab = torch.sqrt(1.0 - a_bar_t)\n",
        "        \n",
        "        # sqrt_ra = √(1/α_t)\n",
        "        sqrt_ra = torch.sqrt(1.0 / (alpha_t + 1e-8))\n",
        "        \n",
        "        # post_var = β_t * (1 - ᾱ_{t-1}) / (1 - ᾱ_t)\n",
        "        # a_bar_t[1:] is ᾱ_t for t=1..T, a_bar_t[:-1] is ᾱ_{t-1} for t=1..T\n",
        "        post_var = beta_t * (1.0 - a_bar_t[:-1]) / (1.0 - a_bar_t[1:] + 1e-8)\n",
        "        \n",
        "        # Store as attributes (Diffusion is not nn.Module)\n",
        "        self.beta = beta_t\n",
        "        self.alpha = alpha_t\n",
        "        self.a_bar = a_bar_t\n",
        "        self.sqrt_ab = sqrt_ab\n",
        "        self.sqrt_1mab = sqrt_1mab\n",
        "        self.sqrt_ra = sqrt_ra\n",
        "        self.post_var = post_var\n",
        "\n",
        "    def to_(self, d: torch.device):\n",
        "        for k,v in vars(self).items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                setattr(self, k, v.to(d))\n",
        "        return self\n",
        "\n",
        "    def q_sample(self, x0: torch.Tensor, t: torch.Tensor, eps: torch.Tensor=None):\n",
        "        r\"\"\"\n",
        "        ### TODO: forward noising\n",
        "          x_t = √(ᾱ_t) * x0 + √(1 - ᾱ_t) * ε,   ε~N(0,I)\n",
        "        Return (x_t, ε). Use provided ε if not None.\n",
        "        \"\"\"\n",
        "        # ====== Implement here ======\n",
        "        if eps is None:\n",
        "            eps = torch.randn_like(x0)\n",
        "        \n",
        "        sqrt_ab_t = self.sqrt_ab[t].view(-1, 1, 1, 1)\n",
        "        sqrt_1mab_t = self.sqrt_1mab[t].view(-1, 1, 1, 1)\n",
        "        \n",
        "        x_t = sqrt_ab_t * x0 + sqrt_1mab_t * eps\n",
        "        return x_t, eps\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# EMA (GIVEN)\n",
        "# ----------------------------\n",
        "class EMA:\n",
        "    def __init__(self, model: nn.Module, decay=0.999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {k: v.detach().clone() for k,v in model.state_dict().items()}\n",
        "    @torch.no_grad()\n",
        "    def update(self, model: nn.Module):\n",
        "        for k, v in model.state_dict().items():\n",
        "            if v.dtype.is_floating_point:\n",
        "                self.shadow[k].mul_(self.decay).add_(v, alpha=1.0 - self.decay)\n",
        "            else:\n",
        "                self.shadow[k] = v.detach().clone()\n",
        "    def copy_to(self, model: nn.Module):\n",
        "        model.load_state_dict(self.shadow, strict=True)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Pred space helpers & DDIM step\n",
        "# ----------------------------\n",
        "def to_eps_from_pred(pred_type, out, xt, sqrt_ab_t, sqrt_1mab_t):\n",
        "    r\"\"\"\n",
        "    ### TODO: map network output 'out' to (ε̂, x̂0) depending on prediction space.\n",
        "\n",
        "      If pred_type == 'eps':\n",
        "        ε̂   = out\n",
        "        x̂0  = (xt - √(1-ᾱ_t)*ε̂)/√(ᾱ_t)\n",
        "\n",
        "      If pred_type == 'x0':\n",
        "        x̂0  = out\n",
        "        ε̂   = (xt - √(ᾱ_t)*x̂0)/√(1-ᾱ_t)\n",
        "\n",
        "      If pred_type == 'v':\n",
        "        x̂0  = √(ᾱ_t)*xt - √(1-ᾱ_t)*out\n",
        "        ε̂   = √(1-ᾱ_t)*xt + √(ᾱ_t)*out\n",
        "    \"\"\"\n",
        "    # ====== Implement here ======\n",
        "    if pred_type == \"eps\":\n",
        "        eps_hat = out\n",
        "        x0_hat = (xt - sqrt_1mab_t * eps_hat) / (sqrt_ab_t + 1e-8)\n",
        "    elif pred_type == \"x0\":\n",
        "        x0_hat = out\n",
        "        eps_hat = (xt - sqrt_ab_t * x0_hat) / (sqrt_1mab_t + 1e-8)\n",
        "    else:  # 'v'\n",
        "        x0_hat = sqrt_ab_t * xt - sqrt_1mab_t * out\n",
        "        eps_hat = sqrt_1mab_t * xt + sqrt_ab_t * out\n",
        "    \n",
        "    return eps_hat, x0_hat\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ddim_p_step(net, dd, x_t, m, y, t, t_prev, pred_type=\"v\", self_cond=False, x0_sc=None, eta=0.0, coord_conv=False):\n",
        "    r\"\"\"\n",
        "    ### TODO: DDIM deterministic step (η=0) with optional noise (η>0):\n",
        "      ā_t = ᾱ_t, ā_{t'} = ᾱ_{t_prev} [or 1 if t_prev=None]\n",
        "      σ_t = η * sqrt( (1 - ā_{t'})/(1 - ā_t) * (1 - ā_t / ā_{t'}) )\n",
        "      x_{t'} = √(ā_{t'}) * x̂0 + √(1 - ā_{t'} - σ_t^2) * ε̂  + 1_{η>0} * σ_t * z\n",
        "\n",
        "    Inputs include conditioning channels [x_t, m, y, (x0_sc), (coords?)].\n",
        "    Return (x_{t'}, x̂0).\n",
        "    \"\"\"\n",
        "    # ====== Implement here ======\n",
        "    B, _, H, W = x_t.shape\n",
        "    device = x_t.device\n",
        "    \n",
        "    # Get ᾱ_t and ᾱ_{t'}\n",
        "    a_bar_t = dd.a_bar[t]  # (B,)\n",
        "    if t_prev is not None:\n",
        "        a_bar_t_prev = dd.a_bar[t_prev]  # (B,)\n",
        "    else:\n",
        "        a_bar_t_prev = torch.ones_like(a_bar_t)  # ᾱ_0 = 1\n",
        "    \n",
        "    # Prepare input to network\n",
        "    din = [x_t, m, y]\n",
        "    if self_cond:\n",
        "        if x0_sc is None:\n",
        "            x0_sc = torch.zeros_like(x_t)\n",
        "        din.append(x0_sc)\n",
        "    if coord_conv:\n",
        "        yy, xx = torch.meshgrid(\n",
        "            torch.linspace(-1, 1, H, device=device),\n",
        "            torch.linspace(-1, 1, W, device=device),\n",
        "            indexing=\"ij\"\n",
        "        )\n",
        "        coords = torch.stack([xx, yy], dim=0).expand(B, -1, -1, -1)\n",
        "        din.append(coords)\n",
        "    \n",
        "    # Forward pass\n",
        "    out = net(torch.cat(din, dim=1), t)\n",
        "    \n",
        "    # Get sqrt values for current timestep\n",
        "    sqrt_ab_t = dd.sqrt_ab[t].view(-1, 1, 1, 1)\n",
        "    sqrt_1mab_t = dd.sqrt_1mab[t].view(-1, 1, 1, 1)\n",
        "    \n",
        "    # Map to (ε̂, x̂0)\n",
        "    eps_hat, x0_hat = to_eps_from_pred(pred_type, out, x_t, sqrt_ab_t, sqrt_1mab_t)\n",
        "    \n",
        "    # Compute σ_t\n",
        "    a_bar_t_view = a_bar_t.view(-1, 1, 1, 1)\n",
        "    a_bar_t_prev_view = a_bar_t_prev.view(-1, 1, 1, 1)\n",
        "    \n",
        "    sigma_t_sq = eta * eta * (1.0 - a_bar_t_prev_view) / (1.0 - a_bar_t_view + 1e-8) * \\\n",
        "                 (1.0 - a_bar_t_view / (a_bar_t_prev_view + 1e-8))\n",
        "    sigma_t = torch.sqrt(sigma_t_sq.clamp(min=0.0))\n",
        "    \n",
        "    # Compute x_{t'}\n",
        "    sqrt_a_bar_t_prev = torch.sqrt(a_bar_t_prev_view)\n",
        "    sqrt_1m_a_bar_t_prev_m_sigma = torch.sqrt((1.0 - a_bar_t_prev_view - sigma_t_sq).clamp(min=0.0))\n",
        "    \n",
        "    x_t_prev = sqrt_a_bar_t_prev * x0_hat + sqrt_1m_a_bar_t_prev_m_sigma * eps_hat\n",
        "    \n",
        "    # Add noise if η > 0\n",
        "    if eta > 0:\n",
        "        z = torch.randn_like(x_t)\n",
        "        x_t_prev = x_t_prev + sigma_t * z\n",
        "    \n",
        "    return x_t_prev, x0_hat\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def enforce_known(dd, x_t, y, m, t, z_fixed=None):\n",
        "    r\"\"\"\n",
        "    Per-step data-consistency on known pixels (m=1):\n",
        "      y_t = √(ᾱ_t) * y + √(1-ᾱ_t) * z_t\n",
        "      x_t ← m ⊙ y_t  +  (1-m) ⊙ x_t\n",
        "    If z_fixed is provided, reuse it; otherwise use zeros for stability.\n",
        "    \"\"\"\n",
        "    z = torch.zeros_like(y) if z_fixed is None else z_fixed\n",
        "    y_t = dd.sqrt_ab[t].view(-1,1,1,1) * y + dd.sqrt_1mab[t].view(-1,1,1,1) * z\n",
        "    return m * y_t + (1.0 - m) * x_t\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab55b3ba",
      "metadata": {},
      "source": [
        "## 6. Loss & Inference _(GIVEN — do not modify)_\n",
        "\n",
        "Implements training loss and the DDIM inpainting sampler used at test time.\n",
        "\n",
        "- **`p2_weight(a_bar_t, k, gamma)`** — SNR-based P2 reweighting for timestep-balanced training.\n",
        "- **`loss_fn(net, dd, x0, m, y, t, ...)`** — Trains in chosen prediction space (`eps` / `x0` / `v`), uses `dd.q_sample` to form `(x_t, ε)`, optional **self-conditioning** and **coord-conv**, pixelwise **L2** with **hole upweighting** via `hole_weight`, and multiplies by **P2** weights; returns the mean loss.\n",
        "- **`inpaint(net, dd, y, m, ...)`** — DDIM inpainting loop over a timestep grid; supports deterministic (`η=0`) or noisy (`η>0`) updates, optional `init_from_y`, per-step **data consistency** via `enforce_known` repeated `dc_repeats` times, optional fixed noise for DC, and optional self-conditioning across steps.\n",
        "\n",
        "> **Heads-up:** Keep the sampler `pred_type` consistent with training. Lower `steps` if memory is tight; `dc_repeats=2` often improves seam quality; use `η=0` for classic DDIM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def p2_weight(a_bar_t, k=1.0, gamma=1.0):\n",
        "    snr = a_bar_t / (1.0 - a_bar_t + 1e-8)\n",
        "    return torch.pow(k + snr, -gamma)\n",
        "\n",
        "def loss_fn(net, dd, x0, m, y, t, pred_type=\"v\", hole_weight=5.0, p2_k=1.0, p2_gamma=1.0,\n",
        "            self_cond=False, coord_conv=False):\n",
        "    \"\"\"\n",
        "    Training loss (noise prediction in chosen space) with hole upweight + p2.\n",
        "    \"\"\"\n",
        "    xt, eps = dd.q_sample(x0, t)\n",
        "    B, _, H, W = x0.shape\n",
        "\n",
        "    # coord-conv channels\n",
        "    coords = None\n",
        "    if coord_conv:\n",
        "        yy, xx = torch.meshgrid(\n",
        "            torch.linspace(-1, 1, H, device=x0.device),\n",
        "            torch.linspace(-1, 1, W, device=x0.device),\n",
        "            indexing=\"ij\"\n",
        "        )\n",
        "        coords = torch.stack([xx, yy], dim=0).expand(B, -1, -1, -1)  # (B,2,H,W)\n",
        "\n",
        "    # self-conditioning (50%): zero-SC prepass to get x0_sc\n",
        "    if self_cond and (random.random() < 0.5):\n",
        "        din0 = [xt, m, y, torch.zeros_like(x0)]\n",
        "        if coords is not None: din0.append(coords)\n",
        "        out0 = net(torch.cat(din0, dim=1), t)\n",
        "        sqrt_ab_t   = dd.sqrt_ab[t].view(-1,1,1,1)\n",
        "        sqrt_1mab_t = dd.sqrt_1mab[t].view(-1,1,1,1)\n",
        "        _, x0_hat0 = to_eps_from_pred(pred_type, out0, xt, sqrt_ab_t, sqrt_1mab_t)\n",
        "        sc = x0_hat0.detach()\n",
        "        din = [xt, m, y, sc]\n",
        "    else:\n",
        "        din = [xt, m, y] + ([torch.zeros_like(x0)] if self_cond else [])\n",
        "\n",
        "    if coords is not None:\n",
        "        din.append(coords)\n",
        "\n",
        "    out = net(torch.cat(din, dim=1), t)\n",
        "\n",
        "    # target in chosen pred space\n",
        "    if pred_type == \"eps\":\n",
        "        target = eps\n",
        "    elif pred_type == \"x0\":\n",
        "        target = x0\n",
        "    else:  # 'v'\n",
        "        target = dd.sqrt_ab[t].view(-1,1,1,1) * eps - dd.sqrt_1mab[t].view(-1,1,1,1) * x0\n",
        "\n",
        "    # pixelwise L2 with hole upweight + p2 weighting\n",
        "    per_pix = (out - target)**2\n",
        "    if hole_weight and hole_weight != 1.0:\n",
        "        weight_mask = 1.0 + (hole_weight - 1.0) * (1.0 - m)\n",
        "        per_pix = per_pix * weight_mask\n",
        "\n",
        "    w = p2_weight(dd.a_bar[t], k=p2_k, gamma=p2_gamma).view(-1,1,1,1)\n",
        "    return (w * per_pix).mean()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def inpaint(net, dd, y, m, steps=50, d=None, dc_repeats=2, dc_fixed_z=True,\n",
        "            pred_type=\"v\", self_cond=False, init_from_y=True,\n",
        "            eta=0.0, coord_conv=False):\n",
        "    device = y.device\n",
        "    tau = torch.linspace(0, dd.T - 1, steps, device=device).round().long()\n",
        "    B = y.size(0)\n",
        "    if init_from_y:\n",
        "        x = m * y + (1.0 - m) * torch.randn_like(y)\n",
        "    else:\n",
        "        x = torch.randn_like(y)\n",
        "    z0 = torch.randn_like(y) if dc_fixed_z else None\n",
        "    x0_sc = None\n",
        "\n",
        "    for i in reversed(range(len(tau))):\n",
        "        ti = tau[i]; tb = ti.repeat(B)\n",
        "        for _ in range(max(1, dc_repeats)):\n",
        "            x = enforce_known(dd, x, y, m, tb, z_fixed=z0)\n",
        "\n",
        "        t_prev = tau[i-1] if i > 0 else None\n",
        "        x, x0_sc = ddim_p_step(net, dd, x, m, y, tb, t_prev, pred_type=pred_type,\n",
        "                               self_cond=self_cond, x0_sc=x0_sc, eta=eta, coord_conv=coord_conv)\n",
        "    x = m * y + (1.0 - m) * x\n",
        "    return x.clamp(-1,1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b0bb0fb",
      "metadata": {},
      "source": [
        "## 7. Train / Sample / Eval helpers _(GIVEN — no argparse)_\n",
        "\n",
        "High-level helpers to **train**, **sample**, and **evaluate** without CLI flags.\n",
        "\n",
        "- **`train(...)`** — Runs full training with AdamW, LR **warmup**, **EMA** tracking, optional **grad accumulation** & **grad clipping**. Periodically saves **inpainting panels** (`outputs/inpaint/panel_*.png`) using the EMA copy, and finally writes a checkpoint to `outputs/inpaint/last.pt` (with config).\n",
        "- **`sample_cmd(...)`** — Loads the checkpoint (prefers **EMA** weights), rebuilds diffusion and UNet from saved config, inpaints a test batch, and saves `outputs/inpaint/samples.png`.\n",
        "- **`eval_cmd(...)`** — Loads the checkpoint (prefers **EMA**), inpaints the MNIST test set up to `n_eval` images, computes **hole-only PSNR/L1**, and writes `results/inpaint_metrics.json`.\n",
        "\n",
        "> **Heads-up:**  \n",
        "> • Keep `pred` consistent between training and sampling.  \n",
        "> • If memory is tight, lower `batch_size`, raise `grad_accum`, or reduce `steps/sample_steps`.  \n",
        "> • Outputs are saved under `outputs/inpaint/` and metrics under `results/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(epochs=3, batch_size=128, lr=2e-4, steps=400,\n",
        "          beta_start=1e-4, beta_end=2e-2, beta_schedule=\"cosine\",\n",
        "          base=96, emb=384, sample_every=400, sample_steps=50,\n",
        "          center_box=12, dc_repeats=2, dc_fixed_z=True,\n",
        "          pred=\"v\", p2_k=1.0, p2_gamma=1.0, hole_weight=5.0,\n",
        "          seed=42, clip_grad=1.0, ema_decay=0.999, warmup_steps=1000,\n",
        "          self_cond=True, eta=0.0, coord_conv=True,\n",
        "          grad_accum:int=1):\n",
        "    seed_all(seed); d = dev()\n",
        "    train_loader, test_loader, (C,H,W) = get_mnist_loader(batch_size=batch_size, num_workers=2)\n",
        "\n",
        "    in_extra = (1 if self_cond else 0) + (2 if coord_conv else 0)\n",
        "    in_ch = 3 + in_extra\n",
        "    net = UNetDeep(in_ch=in_ch, base=base, emb=emb, out_ch=1, dropout=0.1, use_attn=True).to(d)\n",
        "\n",
        "    dd = Diffusion(DCfg(steps=steps, beta_start=beta_start, beta_end=beta_end,\n",
        "                        beta_schedule=beta_schedule)).to_(d)\n",
        "\n",
        "    opt = torch.optim.AdamW(net.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    ema = EMA(net, decay=ema_decay)\n",
        "\n",
        "    step_i = 0\n",
        "    scaler = 1.0 / max(1, grad_accum)\n",
        "    for ep in range(epochs):\n",
        "        running = 0.0\n",
        "        for it,(x,_) in enumerate(tqdm(train_loader, desc=f\"Epoch {ep+1}/{epochs}\")):\n",
        "            x = x.to(d)\n",
        "            B = x.size(0)\n",
        "            m = make_center_box_mask(B,H,W, box=center_box).to(d)\n",
        "            y = m * x\n",
        "            t = torch.randint(0, steps, (B,), device=d, dtype=torch.long)\n",
        "\n",
        "            if warmup_steps and step_i < warmup_steps:\n",
        "                warm_lr = lr * float(step_i + 1) / float(warmup_steps)\n",
        "                for pg in opt.param_groups: pg[\"lr\"] = warm_lr\n",
        "\n",
        "            loss = loss_fn(net, dd, x, m, y, t,\n",
        "                           pred_type=pred, hole_weight=hole_weight,\n",
        "                           p2_k=p2_k, p2_gamma=p2_gamma, self_cond=self_cond,\n",
        "                           coord_conv=coord_conv)\n",
        "            (loss * scaler).backward()\n",
        "            running += float(loss)\n",
        "\n",
        "            if ((it + 1) % grad_accum) == 0:\n",
        "                if clip_grad and clip_grad > 0:\n",
        "                    nn.utils.clip_grad_norm_(net.parameters(), max_norm=clip_grad)\n",
        "                opt.step(); opt.zero_grad(); ema.update(net)\n",
        "                step_i += 1\n",
        "\n",
        "                if (step_i % sample_every) == 0:\n",
        "                    with torch.no_grad():\n",
        "                        net_ema = UNetDeep(in_ch=in_ch, base=base, emb=emb, out_ch=1).to(d)\n",
        "                        ema.copy_to(net_ema); net_ema.eval()\n",
        "                        idx = slice(0, min(16, B))\n",
        "                        comp = inpaint(net_ema, dd, y[idx], m[idx], steps=sample_steps, d=d,\n",
        "                                       dc_repeats=dc_repeats, dc_fixed_z=dc_fixed_z,\n",
        "                                       pred_type=pred, self_cond=self_cond, init_from_y=True,\n",
        "                                       eta=eta, coord_conv=coord_conv)\n",
        "                        panel = torch.cat([x[idx], y[idx], comp], dim=0)\n",
        "                        save_grid(panel, f\"outputs/inpaint/panel_{step_i:06d}.png\", nrow=panel.size(0)//3)\n",
        "                        del net_ema\n",
        "\n",
        "        print(f\"[Epoch {ep+1}] mean loss: {running / max(1,len(train_loader)):.4f}\")\n",
        "\n",
        "    ensure_dir(\"outputs/inpaint\")\n",
        "    torch.save({\n",
        "        \"net\": net.state_dict(),\n",
        "        \"ema\": ema.shadow,\n",
        "        \"cfg\": dict(epochs=epochs, batch_size=batch_size, lr=lr, steps=steps,\n",
        "                    beta_start=beta_start, beta_end=beta_end, beta_schedule=beta_schedule,\n",
        "                    base=base, emb=emb, sample_every=sample_every, sample_steps=sample_steps,\n",
        "                    center_box=center_box, dc_repeats=dc_repeats, dc_fixed_z=dc_fixed_z,\n",
        "                    pred=pred, p2_k=p2_k, p2_gamma=p2_gamma, hole_weight=hole_weight,\n",
        "                    seed=seed, clip_grad=clip_grad, ema_decay=ema_decay, warmup_steps=warmup_steps,\n",
        "                    self_cond=int(self_cond), eta=eta, coord_conv=int(coord_conv),\n",
        "                    grad_accum=grad_accum)},\n",
        "               \"outputs/inpaint/last.pt\")\n",
        "\n",
        "    # Final panel with EMA\n",
        "    with torch.no_grad():\n",
        "        _, test_loader, _ = get_mnist_loader(batch_size=16, num_workers=0)\n",
        "        x,_ = next(iter(test_loader))\n",
        "        x = x.to(d); B = min(16, x.size(0))\n",
        "        m = make_center_box_mask(B,32,32, box=center_box).to(d)\n",
        "        y = m * x[:B]\n",
        "        net_ema = UNetDeep(in_ch=in_ch, base=base, emb=emb, out_ch=1).to(d)\n",
        "        pkg = safe_torch_load(\"outputs/inpaint/last.pt\", map_location=\"cpu\")\n",
        "        state = pkg.get(\"ema\", pkg.get(\"net\"))\n",
        "        net_ema.load_state_dict(state, strict=False)\n",
        "        net_ema.eval()\n",
        "        comp = inpaint(net_ema, dd, y, m, steps=sample_steps, d=d,\n",
        "                       dc_repeats=dc_repeats, dc_fixed_z=dc_fixed_z,\n",
        "                       pred_type=pred, self_cond=self_cond, init_from_y=True,\n",
        "                       eta=eta, coord_conv=coord_conv)\n",
        "        panel = torch.cat([x[:B], y, comp], dim=0)\n",
        "        save_grid(panel, \"outputs/inpaint/panel_final.png\", nrow=B)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_cmd(ckpt=\"outputs/inpaint/last.pt\", n=16, steps=50,\n",
        "               center_box=12, dc_repeats=2, dc_fixed_z=True,\n",
        "               pred=\"v\", self_cond=True, init_from_y=True,\n",
        "               eta=0.0, coord_conv=True):\n",
        "    d = dev()\n",
        "    pkg = safe_torch_load(ckpt, map_location=\"cpu\")\n",
        "    cfg = pkg.get(\"cfg\", {})\n",
        "    base = cfg.get(\"base\", 96); emb = cfg.get(\"emb\", 384)\n",
        "    dd = Diffusion(DCfg(steps=cfg.get(\"steps\", steps),\n",
        "                        beta_start=cfg.get(\"beta_start\", 1e-4),\n",
        "                        beta_end=cfg.get(\"beta_end\", 2e-2),\n",
        "                        beta_schedule=cfg.get(\"beta_schedule\",\"cosine\"))).to_(d)\n",
        "    in_extra = (1 if int(cfg.get(\"self_cond\", 1)) else 0) + (2 if int(cfg.get(\"coord_conv\", 1)) else 0)\n",
        "    in_ch = 3 + in_extra\n",
        "    net = UNetDeep(in_ch=in_ch, base=base, emb=emb, out_ch=1).to(d)\n",
        "    state = pkg.get(\"ema\", pkg.get(\"net\"))\n",
        "    net.load_state_dict(state, strict=False); net.eval()\n",
        "\n",
        "    _, test_loader, (C,H,W) = get_mnist_loader(batch_size=n, num_workers=0)\n",
        "    x,_ = next(iter(test_loader))\n",
        "    x = x.to(d); B = min(n, x.size(0))\n",
        "    m = make_center_box_mask(B,H,W, box=center_box).to(d)\n",
        "    y = m * x[:B]\n",
        "    comp = inpaint(net, dd, y, m, steps=steps, d=d,\n",
        "                   dc_repeats=dc_repeats, dc_fixed_z=dc_fixed_z,\n",
        "                   pred_type=pred, self_cond=bool(int(cfg.get(\"self_cond\",1))),\n",
        "                   init_from_y=init_from_y, eta=float(cfg.get(\"eta\", 0.0)),\n",
        "                   coord_conv=bool(int(cfg.get(\"coord_conv\",1))))\n",
        "    panel = torch.cat([x[:B], y, comp], dim=0)\n",
        "    save_grid(panel, \"outputs/inpaint/samples.png\", nrow=B)\n",
        "    print(\"Wrote outputs/inpaint/samples.png\")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_cmd(ckpt=\"outputs/inpaint/last.pt\", batch_size=256, n_eval=500, steps=50,\n",
        "             center_box=12, dc_repeats=2, dc_fixed_z=True, pred=\"v\",\n",
        "             self_cond=True, init_from_y=True, eta=0.0,\n",
        "             coord_conv=True, seed=123):\n",
        "    seed_all(seed); d = dev()\n",
        "    pkg = safe_torch_load(ckpt, map_location=\"cpu\")\n",
        "    cfg = pkg.get(\"cfg\", {})\n",
        "    base = cfg.get(\"base\", 96); emb = cfg.get(\"emb\", 384)\n",
        "    dd = Diffusion(DCfg(steps=cfg.get(\"steps\", steps),\n",
        "                        beta_start=cfg.get(\"beta_start\", 1e-4),\n",
        "                        beta_end=cfg.get(\"beta_end\", 2e-2),\n",
        "                        beta_schedule=cfg.get(\"beta_schedule\",\"cosine\"))).to_(d)\n",
        "    in_extra = (1 if int(cfg.get(\"self_cond\", 1)) else 0) + (2 if int(cfg.get(\"coord_conv\", 1)) else 0)\n",
        "    in_ch = 3 + in_extra\n",
        "    net = UNetDeep(in_ch=in_ch, base=base, emb=emb, out_ch=1).to(d)\n",
        "    state = pkg.get(\"ema\", pkg.get(\"net\"))\n",
        "    net.load_state_dict(state, strict=False); net.eval()\n",
        "\n",
        "    _, test_loader, (C,H,W) = get_mnist_loader(batch_size=batch_size, num_workers=2)\n",
        "    tot_psnr, tot_l1, tot_n = 0.0, 0.0, 0\n",
        "    for x,_ in test_loader:\n",
        "        x = x.to(d); B = x.size(0)\n",
        "        m = make_center_box_mask(B,H,W, box=center_box).to(d)\n",
        "        y = m * x\n",
        "        comp = inpaint(net, dd, y, m, steps=steps, d=d,\n",
        "                       dc_repeats=dc_repeats, dc_fixed_z=dc_fixed_z,\n",
        "                       pred_type=pred, self_cond=bool(int(cfg.get(\"self_cond\",1))),\n",
        "                       init_from_y=init_from_y, eta=float(cfg.get(\"eta\", 0.0)),\n",
        "                       coord_conv=bool(int(cfg.get(\"coord_conv\",1))))\n",
        "        tot_psnr += psnr_on_mask(comp, x, m) * B\n",
        "        tot_l1   += l1_on_mask(comp, x, m) * B\n",
        "        tot_n    += B\n",
        "        if tot_n >= n_eval: break\n",
        "    avg_psnr = float(tot_psnr / max(1,tot_n))\n",
        "    avg_l1   = float(tot_l1 / max(1,tot_n))\n",
        "    ensure_dir(\"results\")\n",
        "    with open(\"results/inpaint_metrics.json\", \"w\") as f:\n",
        "        json.dump({\"psnr_hole\": avg_psnr, \"l1_hole\": avg_l1}, f, indent=2)\n",
        "    print(json.dumps({\"psnr_hole\": avg_psnr, \"l1_hole\": avg_l1}, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. How to run (after you finish the TODOs)\n",
        "\n",
        "1. **Implement all TODOs** in **Section 5** (embedding, UNet, cosine betas, diffusion buffers/q_sample, pred mapping, DDIM step).\n",
        "2. **Train** for a few epochs (Colab-friendly config below).\n",
        "3. **Sample** to visualize results.\n",
        "4. **Evaluate** PSNR/L1 on the hole region.\n",
        "\n",
        "> 💡 Tip: Start small for quick iteration — e.g., `epochs=3`, `steps=200`, `sample_steps=25`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35537754",
      "metadata": {},
      "source": [
        "## 9. Train _(run after completing Section 5)_\n",
        "\n",
        "Runs AdamW with EMA and saves:\n",
        "- **Checkpoint:** `outputs/inpaint/last.pt`\n",
        "- **Panels (every `sample_every`):** `outputs/inpaint/panel_*.png`\n",
        "\n",
        "**Edit the config** to fit your GPU (lower `batch_size`, reduce `steps/sample_steps`, or increase `grad_accum` if you hit Out of Memory).  \n",
        "> Note: This cell will error until all **TODOs** in Section 5 are implemented. Keep `pred` consistent with later sampling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train(\n",
        "    epochs=3,\n",
        "    batch_size=128,\n",
        "    lr=2e-4,\n",
        "    steps=400,\n",
        "    sample_every=400,\n",
        "    sample_steps=50,\n",
        "    center_box=12,\n",
        "    dc_repeats=2,\n",
        "    dc_fixed_z=True,\n",
        "    pred=\"v\",\n",
        "    p2_k=1.0,\n",
        "    p2_gamma=1.0,\n",
        "    hole_weight=5.0,\n",
        "    seed=42,\n",
        "    clip_grad=1.0,\n",
        "    ema_decay=0.999,\n",
        "    warmup_steps=1000,\n",
        "    self_cond=True,\n",
        "    eta=0.0,\n",
        "    coord_conv=True,\n",
        "    grad_accum=1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b6afe90",
      "metadata": {},
      "source": [
        "## 10. Sample _(after training)_\n",
        "\n",
        "Loads `outputs/inpaint/last.pt` (prefers **EMA** weights), rebuilds UNet/Diffusion from the saved config, inpaints a small test batch, and writes **`outputs/inpaint/samples.png`**.\n",
        "\n",
        "> Keep `pred` consistent with training. You can adjust `steps`, `center_box`, `eta`, or `init_from_y` for different looks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "sample_cmd(\n",
        "    ckpt=\"outputs/inpaint/last.pt\",\n",
        "    n=16,\n",
        "    steps=50,\n",
        "    center_box=12,\n",
        "    dc_repeats=2,\n",
        "    dc_fixed_z=True,\n",
        "    pred=\"v\",\n",
        "    self_cond=True,\n",
        "    init_from_y=True,\n",
        "    eta=0.0,\n",
        "    coord_conv=True,\n",
        ")\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image(filename=\"outputs/inpaint/samples.png\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb6b0f3",
      "metadata": {},
      "source": [
        "## 11. Evaluate _(hole-only PSNR/L1)_\n",
        "\n",
        "Runs inpainting on the MNIST **test** split up to `n_eval` images, computes **PSNR/L1 over the hole region**, and saves **`results/inpaint_metrics.json`** (also prints the values).\n",
        "\n",
        "> For fair comparison, match `steps` and `center_box` with training; use `η=0` for deterministic DDIM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_cmd(\n",
        "    ckpt=\"outputs/inpaint/last.pt\",\n",
        "    batch_size=256,\n",
        "    n_eval=500,\n",
        "    steps=50,\n",
        "    center_box=12,\n",
        "    dc_repeats=2,\n",
        "    dc_fixed_z=True,\n",
        "    pred=\"v\",\n",
        "    self_cond=True,\n",
        "    init_from_y=True,\n",
        "    eta=0.0,\n",
        "    coord_conv=True,\n",
        "    seed=123,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
